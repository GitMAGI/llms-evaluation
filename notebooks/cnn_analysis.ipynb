{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 08:39:09.627861: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-29 08:39:09.631697: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-29 08:39:09.643983: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743233949.665281   93660 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743233949.670537   93660 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-29 08:39:09.693409: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work-around for loading a module from a parent folder in Jupyter/Notebooks\n",
    "parent_dir = os.path.abspath(os.path.join('..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "from modules.utils import get_convolution_layer_expected_output_info\n",
    "from modules.autoencoders_keras import VariationalAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 10\n",
    "#padding = 'valid'  # No padding to the input\n",
    "padding = 'same'    # Apply padding to the input\n",
    "\n",
    "k=40\n",
    "s=30\n",
    "\n",
    "x_1d = np.random.rand(15, 120, 3)\n",
    "k_1d = (k,)\n",
    "s_1d = (s,)\n",
    "\n",
    "x_2d = np.random.rand(15, 120, 120, 3)\n",
    "k_2d = (k,k)\n",
    "s_2d = (s,s)\n",
    "\n",
    "x_3d = np.random.rand(15, 120, 120, 120, 3)\n",
    "k_3d = (k,k,k)\n",
    "s_3d = (s,s,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "def get_convolution_layer_output_info(input, kernel_shape, stride_shape, n_filters=10, padding='valid'):\n",
    "    n_dims = len(input.shape[1:-1])\n",
    "    if n_dims == 1:\n",
    "        conv = layers.Conv1D(\n",
    "            filters=n_filters,\n",
    "            kernel_size=kernel_shape,\n",
    "            strides=stride_shape,\n",
    "            padding=padding,\n",
    "            name=f\"conv_{n_dims}d_layer\"\n",
    "        )\n",
    "    elif n_dims == 2:\n",
    "        conv = layers.Conv2D(\n",
    "            filters=n_filters,\n",
    "            kernel_size=kernel_shape,\n",
    "            strides=stride_shape,\n",
    "            padding=padding,\n",
    "            name=f\"conv_{n_dims}d_layer\"\n",
    "        )\n",
    "    elif n_dims == 3:\n",
    "        conv = layers.Conv3D(\n",
    "            filters=n_filters,\n",
    "            kernel_size=kernel_shape,\n",
    "            strides=stride_shape,\n",
    "            padding=padding,\n",
    "            name=f\"conv_{n_dims}d_layer\"\n",
    "        )\n",
    "    else:\n",
    "        return (None,),(None,)\n",
    "    y = conv(input)\n",
    "    output_shape = y.shape[1:]\n",
    "\n",
    "    #def get_padding_from_output(input, output, stride, kernel):\n",
    "    #    p = (((output-1)*stride)+kernel-input)/2\n",
    "    #    return int(p)\n",
    "    #padding_shape = tuple([get_padding_from_output(input.shape[1:-1][i], ow, stride_shape[i], kernel_shape[i]) for i, ow in enumerate(output_shape[:-1])])\n",
    "\n",
    "    #padding_shape = tuple([abs(int((((ow-1)*stride_shape[i])+kernel_shape[i]-input.shape[1:-1][i])/2)) for i, ow in enumerate(output_shape[:-1])])\n",
    "\n",
    "    #return output_shape, padding_shape, y\n",
    "    return output_shape, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (Conv 1D): (120, 3)\n",
      "Expected output shape (Conv 1D): (4, 10), padding: (10,)\n",
      "Output shape (Conv 1D): (4, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 08:39:13.592354: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "x = x_1d\n",
    "kernel_shape = k_1d\n",
    "stride_shape = s_1d\n",
    "\n",
    "name = 'Conv 1D'\n",
    "\n",
    "print(f'Input shape ({name}): {x.shape[1:]}')\n",
    "\n",
    "# Understaning the output shape of a Conv layer\n",
    "expected_output_shape, expected_padding_shape = get_convolution_layer_expected_output_info(x, kernel_shape, stride_shape, n_filters, padding)\n",
    "print(f'Expected output shape ({name}): {expected_output_shape}, padding: {expected_padding_shape}')\n",
    "\n",
    "output_shape, y = get_convolution_layer_output_info(x, kernel_shape, stride_shape, n_filters, padding)\n",
    "print(f'Output shape ({name}): {output_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (Conv 2D): (120, 120, 3)\n",
      "Expected output shape (Conv 2D): (4, 4, 10), padding: (10, 10)\n",
      "Output shape (Conv 2D): (4, 4, 10)\n"
     ]
    }
   ],
   "source": [
    "x = x_2d\n",
    "kernel_shape = k_2d\n",
    "stride_shape = s_2d\n",
    "\n",
    "name = 'Conv 2D'\n",
    "\n",
    "print(f'Input shape ({name}): {x.shape[1:]}')\n",
    "\n",
    "# Understaning the output shape of a Conv layer\n",
    "expected_output_shape, expected_padding_shape = get_convolution_layer_expected_output_info(x, kernel_shape, stride_shape, n_filters, padding)\n",
    "print(f'Expected output shape ({name}): {expected_output_shape}, padding: {expected_padding_shape}')\n",
    "\n",
    "output_shape, y = get_convolution_layer_output_info(x, kernel_shape, stride_shape, n_filters, padding)\n",
    "print(f'Output shape ({name}): {output_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (Conv 3D): (120, 120, 120, 3)\n",
      "Expected output shape (Conv 3D): (4, 4, 4, 10), padding: (10, 10, 10)\n",
      "Output shape (Conv 3D): (4, 4, 4, 10)\n"
     ]
    }
   ],
   "source": [
    "x = x_3d\n",
    "kernel_shape = k_3d\n",
    "stride_shape = s_3d\n",
    "\n",
    "name = 'Conv 3D'\n",
    "\n",
    "print(f'Input shape ({name}): {x.shape[1:]}')\n",
    "\n",
    "# Understaning the output shape of a Conv layer\n",
    "expected_output_shape, expected_padding_shape = get_convolution_layer_expected_output_info(x, kernel_shape, stride_shape, n_filters, padding)\n",
    "print(f'Expected output shape ({name}): {expected_output_shape}, padding: {expected_padding_shape}')\n",
    "\n",
    "output_shape, y = get_convolution_layer_output_info(x, kernel_shape, stride_shape, n_filters, padding)\n",
    "print(f'Output shape ({name}): {output_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14, 64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_shape, pad_shape = get_convolution_layer_expected_output_info(np.random.rand(1, 28, 28, 32), (3,3), (2,2), n_filters=64, padding='same')\n",
    "display(out_shape, pad_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_shape, pad_shape = get_convolution_layer_expected_output_info(np.random.rand(1, 14, 14, 64), (3,3), (2,2), n_filters=64, padding='same')\n",
    "display(out_shape, pad_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data, batch_size):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_x_train = np.random.normal(3, 2.5, size=(1000, 28, 28, 1))\n",
    "\n",
    "vae_input_shape = (norm_x_train.shape[1:][0], norm_x_train.shape[1:][1], 1)\n",
    "vae_filters=(32, 64, 64, 64)\n",
    "vae_activations=('relu', 'relu', 'relu', 'relu')\n",
    "vae_kernels=(3, 3, 3, 3)\n",
    "vae_strides=(1, 2, 2, 1)\n",
    "vae_latent_space_dim=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"test_vae\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"test_vae\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">(None, 2)</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">(None, 2)</span>, │       <span style=\"color: #00af00; text-decoration-color: #00af00\">105,220</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">(None, 2)</span>)             │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">120,769</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder (\u001b[38;5;33mFunctional\u001b[0m)            │ (\u001b[38;5;34m(None, 2)\u001b[0m, \u001b[38;5;34m(None, 2)\u001b[0m, │       \u001b[38;5;34m105,220\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m(None, 2)\u001b[0m)             │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder (\u001b[38;5;33mFunctional\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │       \u001b[38;5;34m120,769\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,989</span> (882.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,989\u001b[0m (882.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,989</span> (882.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,989\u001b[0m (882.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vae_model = VariationalAutoencoder(\n",
    "    input_shape=vae_input_shape, \n",
    "    filters=vae_filters, \n",
    "    kernels=vae_kernels, \n",
    "    strides=vae_strides, \n",
    "    activations=vae_activations,\n",
    "    latent_space_dim=vae_latent_space_dim,\n",
    "    model_name=\"test_vae\",\n",
    "    reconstruction_loss_weight=1000\n",
    ")\n",
    "vae_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts for epoch number 1\n",
      "Training Loss is:  -10172.646\n",
      "Training starts for epoch number 2\n",
      "Training Loss is:  -10172.646\n",
      "Training starts for epoch number 3\n",
      "Training Loss is:  -10172.646\n",
      "Training starts for epoch number 4\n",
      "Training Loss is:  -10172.646\n",
      "Training starts for epoch number 5\n",
      "Training Loss is:  -10172.646\n",
      "Training starts for epoch number 6\n",
      "Training Loss is:  -10172.646\n",
      "Training starts for epoch number 7\n",
      "Training Loss is:  -10172.646\n",
      "Training starts for epoch number 8\n",
      "Training Loss is:  -10172.646\n",
      "Training starts for epoch number 9\n",
      "Training Loss is:  -10172.646\n",
      "Training starts for epoch number 10\n",
      "Training Loss is:  -10172.646\n",
      "Training starts for epoch number 11\n",
      "Training Loss is:  -10172.646\n",
      "Training starts for epoch number 12\n",
      "Training Loss is:  -10172.646\n",
      "Training starts for epoch number 13\n",
      "Training Loss is:  -10172.646\n",
      "Training starts for epoch number 14\n",
      "Training Loss is:  -10172.646\n",
      "Training starts for epoch number 15\n",
      "Training Loss is:  -10172.646\n",
      "Training starts for epoch number 16\n",
      "Training Loss is:  -10172.646\n",
      "Training starts for epoch number 17\n",
      "Training Loss is:  -10172.646\n",
      "Training starts for epoch number 18\n",
      "Training Loss is:  -10172.646\n",
      "Training starts for epoch number 19\n",
      "Training Loss is:  -10172.646\n",
      "Training starts for epoch number 20\n",
      "Training Loss is:  -10172.646\n",
      "Training Complete!!!\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "b_size = 128\n",
    "optimizer = tf.keras.optimizers.get(\"Adam\")\n",
    "optimizer.learning_rate.assign(.001)\n",
    "\n",
    "def custom_loss(y_true,y_pred,mean,log_var):\n",
    "  loss_rec = tf.reduce_mean(tf.reduce_sum(tf.keras.losses.binary_crossentropy(y_true,y_pred), axis = (1,2)))\n",
    "  loss_reg = -0.5 * (1 + log_var - tf.square(mean) - tf.exp(log_var))  \n",
    "  return loss_rec+tf.reduce_mean(tf.reduce_sum(loss_reg, axis=1))\n",
    "  \n",
    "@tf.function\n",
    "def training_block(x_batch, encoder_model, decoder_model, vae_model):\n",
    "  with tf.GradientTape() as recorder:\n",
    "    z,mean,log_var = encoder_model(x_batch)\n",
    "    y_pred = decoder_model(z)\n",
    "    y_true = x_batch\n",
    "    loss = custom_loss(y_true,y_pred, mean, log_var)  \n",
    "  partial_derivatives = recorder.gradient(loss, vae_model.trainable_weights)\n",
    "  optimizer.apply_gradients(zip(partial_derivatives, vae_model.trainable_weights))\n",
    "  return loss\n",
    "  \n",
    "def neuralearn(epochs, train_dataset, encoder_model, decoder_model, vae_model):\n",
    "  for epoch in range(1,epochs+1):\n",
    "    print('Training starts for epoch number {}'.format(epoch))\n",
    "    for step, x_batch in enumerate(train_dataset):\n",
    "      loss = training_block(x_batch, encoder_model, decoder_model, vae_model)\n",
    "    print('Training Loss is: ', loss.numpy())\n",
    "  print('Training Complete!!!')\n",
    "\n",
    "neuralearn(epochs, data_generator(norm_x_train, batch_size=b_size), vae_model.get_encoder(), vae_model.get_decoder(), vae_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms-evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
